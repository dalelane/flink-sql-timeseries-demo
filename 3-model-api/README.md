In a real production environment, it would likely be better to use a dedicated model hosting platform such as [watsonx.ai](https://www.ibm.com/docs/en/software-hub/5.2.x?topic=setup-deploying-custom-foundation-models) that can simplify the work involved in running, managing, scaling, and maintaining a custom model and providing a simple inferencing API. 

For this simplified demo, I'm using a custom Python docker image to represent that idea of a hosted inferencing REST API. 
 